{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99af0264-69b5-49f6-b692-b0e084509ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyodbc\n",
    "from collections import defaultdict\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db3a9cf7-5052-42c3-aaa0-0904be6de519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"pipeline_name\", \"\")\n",
    "dbutils.widgets.text(\"pipeline_run_id\", \"\")\n",
    "dbutils.widgets.text(\"Mode\", \"\")\n",
    "dbutils.widgets.text(\"process_name\", \"\")\n",
    "dbutils.widgets.text(\"Table_Names\", \"\")\n",
    "dbutils.widgets.text(\"landing_path\", \"\")\n",
    "dbutils.widgets.text(\"curated_path\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739486b8-c76c-4856-bf71-3a73c098740d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = dbutils.widgets.get(\"pipeline_name\").strip()\n",
    "pipeline_run_id = dbutils.widgets.get(\"pipeline_run_id\").strip()\n",
    "mode=dbutils.widgets.get(\"Mode\")\n",
    "process_name = dbutils.widgets.get(\"process_name\").strip()\n",
    "Table_Names = dbutils.widgets.get(\"Table_Names\").strip()\n",
    "landing_path = dbutils.widgets.get(\"landing_path\").strip(\"/\")\n",
    "curated_path = dbutils.widgets.get(\"curated_path\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f79b0e-3f36-44b5-af89-584c314c1a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PipelineLogger:\n",
    "    def __init__(self, pipeline_name, pipeline_run_id):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.pipeline_run_id = pipeline_run_id\n",
    " \n",
    "    def _connect(self):\n",
    "        server = 'etltask.database.windows.net'\n",
    "        database = 'tasketldb'\n",
    "        username = dbutils.secrets.get(scope='Azr-adf-scope1', key='USERNAME')\n",
    "        password = dbutils.secrets.get(scope='Azr-adf-scope1', key='PASSWORD')\n",
    "        return pyodbc.connect(\n",
    "            f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "        )\n",
    " \n",
    "    def log_start_time(self):\n",
    "        try:\n",
    "            conn = self._connect()\n",
    "            cursor = conn.cursor()\n",
    "            start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO meta_data.pipeline_logs (pipeline_run_id, pipeline_name, start_time)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (self.pipeline_run_id, self.pipeline_name, start_time))\n",
    "            conn.commit()\n",
    "            logger.info(\"successs\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"error\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    " \n",
    "    def log_end_time(self):\n",
    "        try:\n",
    "            conn = self._connect()\n",
    "            cursor = conn.cursor()\n",
    "            end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            cursor.execute(\"\"\"\n",
    "                UPDATE meta_data.pipeline_logs\n",
    "                SET end_time = ?\n",
    "                WHERE pipeline_run_id = ? AND pipeline_name = ?\n",
    "            \"\"\", (end_time, self.pipeline_run_id, self.pipeline_name))\n",
    "            conn.commit()\n",
    "            logger.info(\"success\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"error\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    def load_and_transform_table(self,Table_Names, landing_path, column_meta_by_table):\n",
    "        logger.info(f\"Processing table: {Table_Names}\")\n",
    "        df = spark.read.parquet(landing_path)\n",
    "        mappings = column_meta_by_table.get(Table_Names, [])\n",
    " \n",
    "        sql_to_spark_type = {\n",
    "            \"int\": IntegerType(),\n",
    "            \"string\": StringType(),\n",
    "            \"float\": FloatType(),\n",
    "            \"double\": DoubleType(),\n",
    "            \"date\": DateType(),\n",
    "            \"timestamp\": TimestampType(),\n",
    "            \"varchar(500)\": StringType()\n",
    "        }\n",
    " \n",
    "        for col_map in mappings:\n",
    "            src = col_map[\"source_column_name\"]\n",
    "            dst = col_map[\"destination_column_name\"]\n",
    "            dtype = sql_to_spark_type.get(col_map[\"destination_column_data_type\"], StringType())\n",
    " \n",
    "            if src in df.columns:\n",
    "                df = df.withColumn(src, col(src).cast(dtype))\n",
    "                if src != dst:\n",
    "                    df = df.withColumnRenamed(src, dst)\n",
    "            else:\n",
    "                logger.warning(f\"Column not found in the DataFrame: {src}\")\n",
    "       \n",
    "        df = df.dropDuplicates()\n",
    " \n",
    "        return df\n",
    " \n",
    "# Main method\n",
    "    def run_dqm_validation(self, Table_Names, landing_path, curated_path):\n",
    "        try:\n",
    "            df = spark.read.format(\"parquet\").load(landing_path)\n",
    " \n",
    "            conn = self._connect()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT source_table_name, source_column_name, destination_column_name, destination_column_data_type\n",
    "                FROM meta_data.column_meta\n",
    "            \"\"\")\n",
    "            rows = cursor.fetchall()\n",
    " \n",
    "            column_meta_by_table = defaultdict(list)\n",
    "            for row in rows:\n",
    "                column_meta_by_table[row.source_table_name].append({\n",
    "                    \"source_column_name\": row.source_column_name,\n",
    "                    \"destination_column_name\": row.destination_column_name,\n",
    "                    \"destination_column_data_type\": row.destination_column_data_type.lower()\n",
    "                })\n",
    " \n",
    "            transformed_df = self.load_and_transform_table(Table_Names, landing_path, column_meta_by_table)\n",
    " \n",
    "            output_path = f\"{curated_path}/{Table_Names}\"\n",
    "            transformed_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    " \n",
    "            logger.info(\"Success\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DQM Failed: {str(e)}\")\n",
    "   \n",
    "    def archive_path(self, landing_path, Table_Names):\n",
    "        try: \n",
    "            dst_dir = f\"dbfs:/mnt/Etltask/archive/{Table_Names}\"  \n",
    "            logger.info(f\"Moving directory {landing_path} to {dst_dir}\")\n",
    " \n",
    "        # Move the whole directory recursively\n",
    "            dbutils.fs.mv(landing_path, dst_dir, recurse=True)\n",
    " \n",
    "            logger.info(f\"Successfully moved {landing_path} to {dst_dir}\")\n",
    " \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to move directory {Table_Names}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7b05af-5055-4a19-86f6-94072eaeb627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    function= PipelineLogger(pipeline_name, pipeline_run_id)\n",
    " \n",
    "    try:\n",
    "        if process_name == \"start_time\":\n",
    "            function.log_start_time()\n",
    "        elif process_name == \"l2c\":\n",
    "            function.run_dqm_validation(Table_Names, landing_path, curated_path)\n",
    "        elif process_name == \"end_time\":\n",
    "            function.log_end_time()\n",
    "        elif process_name == \"archive\":\n",
    "            function.archive_path(landing_path, Table_Names)\n",
    "        else:\n",
    "            logger.info(\"Invalid mode. Use: start, dqm, end.\")\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Pipeline failed with error: {ex}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Etl_nb",
   "widgets": {
    "File_Names": {
     "currentValue": "",
     "nuid": "79f71f15-bc40-4f15-ae56-07368fa9bcee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "File_Names",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "File_Names",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "curated_path": {
     "currentValue": "",
     "nuid": "6b6da817-304e-46f9-b825-ee65906b798f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "curated_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "curated_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "landing_path": {
     "currentValue": "",
     "nuid": "c2bd25ac-da6c-43d8-ab25-9383a8c7db78",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "landing_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "landing_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "pipeline_name": {
     "currentValue": "",
     "nuid": "6e9941e2-90c2-4d82-a1fc-e69d39865c65",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pipeline_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pipeline_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "pipeline_run_id": {
     "currentValue": "",
     "nuid": "eb0e3952-6aa4-4de4-9602-d8b8b64722e2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pipeline_run_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pipeline_run_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "process_name": {
     "currentValue": "",
     "nuid": "48dec8ec-5f3c-4175-997c-b018985318ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "process_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "process_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
